>>> Training epoch 0
Epoch 0: Train Loss: 3.7523
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.0502716228874138
BLEU: {'Bleu_1': 0.19252873563163067, 'Bleu_2': 0.06390573430469455, 'Bleu_3': 2.3567219944933994e-07, 'Bleu_4': 4.5935075403208735e-10}
>>> Training epoch 1
Epoch 1: Train Loss: 3.9692
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.031821439676564485
BLEU: {'Bleu_1': 0.17013888888829815, 'Bleu_2': 0.04347909956233992, 'Bleu_3': 1.9575829811218668e-07, 'Bleu_4': 4.231420357771374e-10}
>>> Training epoch 2
Epoch 2: Train Loss: 3.7645
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.016047950968967557
BLEU: {'Bleu_1': 0.1541666666660243, 'Bleu_2': 0.04564354645856596, 'Bleu_3': 2.1695826460990928e-07, 'Bleu_4': 4.840652567401287e-10}
>>> Training epoch 3
Epoch 3: Train Loss: 3.4351
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.030790915591754817
BLEU: {'Bleu_1': 0.17521367521292644, 'Bleu_2': 0.05696224792889802, 'Bleu_3': 2.5400154000216295e-07, 'Bleu_4': 5.492994144021014e-10}
>>> Training epoch 4
Epoch 4: Train Loss: 3.0689
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.020236461218913013
BLEU: {'Bleu_1': 0.13690010598331298, 'Bleu_2': 0.027191761708080887, 'Bleu_3': 1.6460806441105422e-07, 'Bleu_4': 4.177855765712513e-10}
>>> Training epoch 5
Epoch 5: Train Loss: 2.6718
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.02638328001059737
BLEU: {'Bleu_1': 0.17948717948641246, 'Bleu_2': 0.04992872412605093, 'Bleu_3': 2.32636889321116e-07, 'Bleu_4': 5.142695498509456e-10}
>>> Training epoch 6
Epoch 6: Train Loss: 2.2670
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.017091023940651998
BLEU: {'Bleu_1': 0.13684210526171744, 'Bleu_2': 8.919605569943834e-10, 'Bleu_3': 1.728715501740185e-12, 'Bleu_4': 7.850691608810027e-14}
>>> Training epoch 7
