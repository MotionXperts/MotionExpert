finetune---------------------------------------------------------------

>>> Training epoch 0
Epoch 0: Train Loss: 3.7729
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 1
Epoch 1: Train Loss: 3.9784
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 2
Epoch 2: Train Loss: 3.7794
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 3
Epoch 3: Train Loss: 3.4377
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 4
Epoch 4: Train Loss: 3.1014
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 5
Epoch 5: Train Loss: 2.6747
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 6
Epoch 6: Train Loss: 2.2862
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 7
Epoch 7: Train Loss: 1.9028
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 8
Epoch 8: Train Loss: 1.5791
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
>>> Training epoch 9
Epoch 9: Train Loss: 1.3230
None
{'input_ids': tensor([[148,   1,   0],
        [148,  33,   1],
        [  1,   0,   0]]), 'attention_mask': tensor([[1, 1, 0],
        [1, 1, 1],
        [1, 0, 0]])}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
decoder_input_ids torch.Size([2, 0])
CIDEr: 0.0
BLEU: {'Bleu_1': 0.0, 'Bleu_2': 0.0, 'Bleu_3': 0.0, 'Bleu_4': 0.0}
