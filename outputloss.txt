>>> Training epoch 0
Epoch 0: Train Loss: 3.7418
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.05536550679956799
BLEU: {'Bleu_1': 0.1944444444437693, 'Bleu_2': 0.060006858318376, 'Bleu_3': 2.4266124020362627e-07, 'Bleu_4': 4.971029555964597e-10}
>>> Training epoch 1
