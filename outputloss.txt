>>> Training epoch 0
Epoch 0: Train Loss: 3.7446
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.03416163736478791
BLEU: {'Bleu_1': 0.17197522095015536, 'Bleu_2': 9.621226634017828e-10, 'Bleu_3': 1.7869299208383529e-12, 'Bleu_4': 8.008939515091982e-14}
>>> Training epoch 1
Epoch 1: Train Loss: 3.9899
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.03893627687926305
BLEU: {'Bleu_1': 0.2060297217461691, 'Bleu_2': 0.03450031550066345, 'Bleu_3': 1.9856083229818564e-07, 'Bleu_4': 4.944361043110978e-10}
>>> Training epoch 2
Epoch 2: Train Loss: 3.7401
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.03718374212906628
BLEU: {'Bleu_1': 0.16480360214157014, 'Bleu_2': 0.040316485549136194, 'Bleu_3': 2.0783435196147425e-07, 'Bleu_4': 4.872075345605939e-10}
>>> Training epoch 3
Epoch 3: Train Loss: 3.4458
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.05061608275333588
BLEU: {'Bleu_1': 0.18636848944565113, 'Bleu_2': 0.061122905345389854, 'Bleu_3': 2.5305169344011764e-07, 'Bleu_4': 5.25992685871784e-10}
>>> Training epoch 4
Epoch 4: Train Loss: 3.0531
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.04795525091746325
BLEU: {'Bleu_1': 0.15911592465931262, 'Bleu_2': 0.0557603176450871, 'Bleu_3': 2.570064360586056e-07, 'Bleu_4': 5.693021119111557e-10}
>>> Training epoch 5
Epoch 5: Train Loss: 2.6747
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.04682460472724765
BLEU: {'Bleu_1': 0.17147779363661908, 'Bleu_2': 0.06222333762569279, 'Bleu_3': 2.916689599311111e-07, 'Bleu_4': 6.548579711633066e-10}
>>> Training epoch 6
Epoch 6: Train Loss: 2.2670
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.042119709190420455
BLEU: {'Bleu_1': 0.1556286458958453, 'Bleu_2': 0.05261208489457845, 'Bleu_3': 2.675219050053897e-07, 'Bleu_4': 6.313811007470088e-10}
>>> Training epoch 7
Epoch 7: Train Loss: 1.8976
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.04435248336481755
BLEU: {'Bleu_1': 0.16113622421082444, 'Bleu_2': 0.05356179266136182, 'Bleu_3': 2.707038300710834e-07, 'Bleu_4': 6.366640463286567e-10}
>>> Training epoch 8
Epoch 8: Train Loss: 1.5916
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.04912852350357878
BLEU: {'Bleu_1': 0.1529798630609425, 'Bleu_2': 0.05235845254334455, 'Bleu_3': 2.662458596209066e-07, 'Bleu_4': 6.257507730315826e-10}
>>> Training epoch 9
Epoch 9: Train Loss: 1.3118
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
decoder_input_ids torch.Size([2, 1])
CIDEr: 0.052070949985622374
BLEU: {'Bleu_1': 0.1523141067884143, 'Bleu_2': 0.05198124483751011, 'Bleu_3': 2.6342103280351467e-07, 'Bleu_4': 6.165011531103368e-10}
